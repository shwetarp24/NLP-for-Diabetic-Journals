{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Latent Dirichlet Analysis: LDA assumes documents are produced from a mixture of topics. Those topics then generate words \n",
    "#based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics\n",
    "#would create those documents in the first place. \n",
    "#LDA converts this Document-Term Matrix into two lower dimensional matrices – M1 and M2. - M1 is a document-topics matrix and \n",
    "#- M2 is a topic – terms matrix with dimensions (N,  K) and (K, M) respectively, where   - N is the number of documents,  \n",
    "#- K is the number of topics   - M is the vocabulary size. \n",
    "#For every topic, two probabilities p1 and p2 are calculated. \n",
    "#P1 – p(topic t / document d) = the proportion of words in document d that are currently assigned to topic t. \n",
    "#P2 – p(word w / topic t) = the proportion of assignments to topic t over all documents that come from this word w. \n",
    "#After many iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. \n",
    "#This is the convergence point of LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Alpha and Beta Hyperparameters  alpha represents document-topic density and Beta represents topic-word density. \n",
    "#Higher the value of alpha, documents are composed of more topics and lower the value of alpha, documents contain fewer topics.\n",
    "#On the other hand, higher the beta, topics are composed of many words in the corpus, and with the lower value of beta, \n",
    "#they are composed of few words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A common reason for such a charade is that we want to determine similarity between pairs of documents, or the similarity \n",
    "#between a specific document and a set of other documents (such as a user query vs. indexed documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = [\"Human machine interface for lab abc computer applications human\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]\n",
    "#raw_corpus1 = open('C:/Users/lohitaksh/Desktop/R/HealthNews/Dentistry00501.txt', 'r')\n",
    "#raw_corpus2 = open('C:/Users/lohitaksh/Desktop/R/HealthNews/Dentistry00502.txt', 'r')\n",
    "#raw_corpus3 = open('C:/Users/lohitaksh/Desktop/R/HealthNews/Dentistry00503.txt', 'r')\n",
    "#raw_corpus = open('C:/Users/lohit/Desktop/course/KDD/HealthNews/Dentistry00504.txt', 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #we first use this tiny corpus to define a 2-dimensional LSI space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer', 'human'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in raw_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 35 features, 51 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('C:/Users/lohit/AppData/Local/Temp/den.dict')\n",
    "corpus = corpora.MmCorpus('C:/Users/lohit/AppData/Local/Temp/den.mm') # comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All the text documents combined is known as the corpus. To run any mathematical model on text corpus, it is a good practice\n",
    "#to convert it into a matrix representation. LDA model looks for repeating term patterns in the entire DT matrix. \n",
    "#Python provides many great libraries for text mining practices, “gensim” is one such clean and beautiful library to handle\n",
    "#text data. It is scalable, robust and efficient. Following code shows how to convert a corpus into a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1)],\n",
       " [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(1, 1), (4, 1), (5, 1), (8, 1)],\n",
       " [(0, 1), (5, 2), (8, 1)],\n",
       " [(4, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(3, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_corpus]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We see that the mode has come up with probabilities associated with the words. Each line is a topic with individual \n",
    "#topic terms and weights. Topic1 science , and Topic2 can be termed as education and Topic3 could be name as family and so on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next step is to create an object for LDA model and train it on Document-Term matrix. The training also requires few parameters\n",
    "#as input which are explained in the above section. The gensim module allows both LDA model estimation from a training corpus\n",
    "#and inference of topic distribution on new, unseen documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x2052e573898>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.058*\"that\" + 0.043*\"is\" + 0.031*\"mucins\" + 0.028*\"on\" + 0.028*\"salivary\"'), (1, '0.013*\"thought\" + 0.013*\"more\" + 0.013*\"them\" + 0.013*\"saliva\" + 0.013*\"cavities\"'), (2, '0.013*\"thought\" + 0.013*\"more\" + 0.013*\"them\" + 0.013*\"saliva\" + 0.013*\"against\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Each line is a topic with individual topic terms and weights. Topic1 can be termed as Bad Health, and Topic3 can be termed\n",
    "#as Family."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
