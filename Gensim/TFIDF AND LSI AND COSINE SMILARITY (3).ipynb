{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gensim accomplishes this by taking a corpus, a collection of text documents, and producing a vector representation of\n",
    "#the text in the corpus. The vector representation can then be used to train a model, which is an algorithm to create \n",
    "#different representations of the data, which are usually more semantic. These three concepts are key to understanding \n",
    "#how Gensim works. At the same time, we'll work through a simple example that illustrates each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim Small example \n",
    "# Small corpus for this example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_corpus = [\"Human machine interface for lab abc computer applications human\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A corpus is a collection of digital documents. This corpus is fed to Gensim from which it will infer the structure of the \n",
    "#documents and extract topics from the documents. Once the algorithm learns on how to infer topics from the training corpus, \n",
    "#it can be used to assign topics to new documents which were not present in the training corpus.For this reason, we also refer\n",
    "#to this collection as the training corpus. No human intervention is required - the topic classification is unsupervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Human machine interface for lab abc computer applications human', 'A survey of user opinion of computer system response time', 'The EPS user interface management system', 'System and human system engineering testing of EPS', 'Relation of user perceived response time to error measurement', 'The generation of random binary unordered trees', 'The intersection graph of paths in trees', 'Graph minors IV Widths of trees and well quasi ordering', 'Graph minors A survey']\n"
     ]
    }
   ],
   "source": [
    "print(raw_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just one 10-second kiss transfers 80 million bacteria\n",
      " <header>In the 1960s, a singer named Betty Everett belted, \"If you wanna know if he loves you so, it's in his kiss!\" Covered by Cher in the 1990s, the song neglects to mention what is also \"in his kiss\" - 80 million bacteria, according to a new study published in the journal <em>Microbiome</em>.</header><div class=\"photobox_right\" style='max-width:350px;' ><img src=\"http://www.medicalnewstoday.com/images/articles/285/285563/couple-kissing.jpg\" alt=\"Couple kissing\"><br><em>What is \"in his kiss\"? According to the latest study, 80 million bacteria.</em></div><p>Before germaphobes swear off kissing forever, it should be noted that over 100 trillion microorganisms naturally live in our bodies. Called the microbiome, they are vital for digesting food, synthesizing nutrients and preventing disease.</p><p>The researchers - led by Remco Kort, of TNO (Netherlands Organisation for Applied Scientific Research) and adviser to the Micropia museum of microbes in the Netherlands - note that a number of different factors are important for shaping our individual microbiota, including genetic relatedness, diet and age.</p><p>But our surroundings - including the individuals with whom we interact - also affect our microbiota. The team notes that our mouths themselves contain over 700 varieties of bacteria, and these are also influenced by those we are closest to - particularly our romantic partners.</p><p>Kort says that as far as he and his colleagues know, \"the exact effects of intimate kissing on the oral microbiota have never been studied. We wanted to find out the extent to which partners share their oral microbiota, and it turns out, the more a couple kiss, the more similar they are.\"</p><h2>Probiotic bacteria in partner's mouth rose threefold after kiss</h2><p>To further study how kissing affects oral microbiota, Kort and his team assessed 21 couples who completed questionnaires on their kissing behavior, including average intimate kiss frequency. </p><p>\"Intimate kissing involves full tongue contact, and saliva exchange appears to be a courtship behavior unique to humans and is common in over 90% of known cultures,\" he says.</p><p>The researchers cite a recent study detailing the importance of kissing in human mating, which proposes that the \"first kiss\" serves to assess a potential mate. They add:</p><blockquote><p>\"Kissing may contribute in mate assessment and bonding via sampling of chemical taste cues in the saliva, including those resulting from the metabolic activity of the bacterial community on the surface of the tongue.\"</p></blockquote><p>After taking swab samples to determine the composition of each individual's oral microbiota on the tongue and in the saliva, the researchers found that when couples intimately kiss at high frequencies, their salivary microbiota become similar. In fact, nine intimate kisses per day was linked to couples having \"significantly shared salivary microbiota.\"</p><p>In order to quantify bacteria transfer, one individual from each couple drank a probiotic beverage with specific varieties of bacteria called <em>Lactobacillus</em> and <em>Bifidobacteria</em>.</p><p><strong>Results showed that after kissing intimately, the quantity of probiotic bacteria in the other individual's saliva rose threefold, and during a 10-second kiss, a total of 80 million bacteria were transferred.</strong></p><p>\"This study indicates that a shared salivary microbiota requires a frequent and recent bacterial exchange and is therefore most pronounced in couples with relatively high intimate kiss frequencies,\" write the authors.</p><h2>Saliva bacteria 'only transiently present,' while tongue bacteria stay long term</h2><p>Another finding from the study reveals an essential role for mechanisms behind selection of oral microbiota - particularly those on the tongue. Though tongue microbiota were more similar among partners than unrelated individuals, the similarity was not altered with more frequent kissing, which is in contrast to microbiota found in saliva.</p><p>The researchers further explain:</p><blockquote><p>Ã¢â‚¬Â¨\"Our findings suggest that the shared microbiota among partners is able to proliferate in the oral cavity, but the collective bacteria in the saliva are only transiently present and eventually washed out, while those on the tongue's surface found a true niche, allowing long-term colonization.\"</p></blockquote></p><p>To calculate the number of bacteria that are transferred during a kiss, the researchers used average transfer values and assumptions related to bacterial transfer, kiss contact surface and the value for average volume of saliva.</p><p><em>Medical News Today</em> recently reported on a study that suggested  saliva protects teeth against cavities more than we previously thought.</p><p id='slidebox_trigger'><strong>Written by</strong> Marie Ellis<br/></p> \n",
      "A new study examining oral microbiota finds that when couples kiss intimately at high frequencies, their salivary microbiota become similar.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_corpus1 = open(r'C:\\Users\\lohit\\Desktop\\course\\KDD\\HealthNews\\Dentistry00501.txt', 'r')\n",
    "print(raw_corpus1.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We first remove all the words the commonly used English words - called stop words such as 'the', ‘a’, ‘we’, etc.) and words \n",
    "#that occur only once in the corpus.\n",
    "# Second we are counting number of different words inside the document and if the count is more than one it will keep on adding\n",
    "# frequency and calcuate the total frequency of all words.\n",
    "# Now we are take the words which are more than 0 time inside the corpus.\n",
    "# After that we are printing the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human',\n",
       "  'machine',\n",
       "  'interface',\n",
       "  'lab',\n",
       "  'abc',\n",
       "  'computer',\n",
       "  'applications',\n",
       "  'human'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in raw_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 0] for text in texts]\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We now need to tokenize our data. This breaks the documents into words and assigns tokens: unique numbers to the words that\n",
    "#have been repeated more than “x” times. Thus, we associate each word in the corpus with a unique integer ID. We can do this \n",
    "#using the Gensim.corpora.Dictionary class. This dictionary defines the vocabulary of all words that our processing knows about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:25,961 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-10-25 18:24:25,965 : INFO : built Dictionary(35 unique tokens: ['human', 'machine', 'interface', 'lab', 'abc']...) from 9 documents (total 53 corpus positions)\n",
      "2017-10-25 18:24:25,969 : INFO : saving Dictionary object under C:/Users/lohit/AppData/Local/Temp/den.dict, separately None\n",
      "2017-10-25 18:24:25,975 : INFO : saved C:/Users/lohit/AppData/Local/Temp/den.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35 unique tokens: ['human', 'machine', 'interface', 'lab', 'abc']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "dictionary.save('C:/Users/lohit/AppData/Local/Temp/den.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next, we need to represent the documents mathematically to be able to continue further processing, so we represent each \n",
    "#document as a vector. We use the bag-of-words model where each document is represented by a vector containing the frequency \n",
    "#counts of each word in the dictionary. The length of the vector is the number of entries in the dictionary. One of the main \n",
    "#properties of the bag-of-words model is that it completely ignores the order of the tokens in the document that is encoded,\n",
    "#hence bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human': 0, 'machine': 1, 'interface': 2, 'lab': 3, 'abc': 4, 'computer': 5, 'applications': 6, 'survey': 7, 'user': 8, 'opinion': 9, 'system': 10, 'response': 11, 'time': 12, 'eps': 13, 'management': 14, 'engineering': 15, 'testing': 16, 'relation': 17, 'perceived': 18, 'error': 19, 'measurement': 20, 'generation': 21, 'random': 22, 'binary': 23, 'unordered': 24, 'trees': 25, 'intersection': 26, 'graph': 27, 'paths': 28, 'minors': 29, 'iv': 30, 'widths': 31, 'well': 32, 'quasi': 33, 'ordering': 34}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now are trying some small example and make vector for them to get the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"found when among transfer saliva\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1), (13, 1), (17, 1)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"EPS computer relation\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count\n",
    "#of this token. Now changing the whole courpus into vector form.\n",
    "#We are saving this into our temporary folder called with an extension of “.mm”. Note that while this list lives entirely\n",
    "#in memory, in most applications you will want a more scalable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:31,112 : INFO : storing corpus in Matrix Market format to C:/Users/lohit/AppData/Local/Temp/den.mm\n",
      "2017-10-25 18:24:31,116 : INFO : saving sparse matrix to C:/Users/lohit/AppData/Local/Temp/den.mm\n",
      "2017-10-25 18:24:31,118 : INFO : PROGRESS: saving document #0\n",
      "2017-10-25 18:24:31,121 : INFO : saved 9x35 matrix, density=16.190% (51/315)\n",
      "2017-10-25 18:24:31,126 : INFO : saving MmCorpus index to C:/Users/lohit/AppData/Local/Temp/den.mm.index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(5, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(2, 1), (8, 1), (10, 1), (13, 1), (14, 1)],\n",
       " [(0, 1), (10, 2), (13, 1), (15, 1), (16, 1)],\n",
       " [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
       " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
       " [(25, 1), (26, 1), (27, 1), (28, 1)],\n",
       " [(25, 1), (27, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)],\n",
       " [(7, 1), (27, 1), (29, 1)]]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "corpora.MmCorpus.serialize('C:/Users/lohit/AppData/Local/Temp/den.mm', bow_corpus)\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now that we have vectorized our corpus we can begin to transform it using models. We use model as an abstract term \n",
    "#referring to a transformation from one document representation to another. In Gensim documents are represented as vectors\n",
    "#so a model can be thought of as a transformation between two vector spaces. The details of this transformation are learned \n",
    "#from the training corpus. One simple example of a model is tf-idf. The tf-idf model transforms vectors from the bag-of-words\n",
    "#representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in \n",
    "#the corpus.\n",
    "#Term Frequency, Inverse Document Frequency (TF-IDF) TF-IDF is a way to score the importance of words (or \"terms\") in a \n",
    "#document based on how frequently they appear across multiple documents.\n",
    "#If a word appears frequently in a document, it's important and TF-IDF gives the word a high score.\n",
    "#But if a word appears in many documents, then it's not a unique identifier and gives the word a low score.\n",
    "#Therefore, stop words will be scaled down. Words that appear frequently in a single document will be scaled up.\n",
    "#For a term t in a document d, the weight Wt, d of term t in document d is given by:\n",
    "#Wt,d = TFt,d log (N/DFt )\n",
    "#Where,\n",
    "#TFt,d is the number of occurrences of t in document d.\n",
    "#DFt is the number of documents containing term t.\n",
    "#N is the total number of documents in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:32,547 : INFO : collecting document frequencies\n",
      "2017-10-25 18:24:32,549 : INFO : PROGRESS: processing document #0\n",
      "2017-10-25 18:24:32,551 : INFO : calculating IDF weights for 9 documents and 34 features (51 matrix non-zeros)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.5773502691896257), (5, 0.5773502691896257), (13, 0.5773502691896257)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import models\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "# transform the \"system minors\" string\n",
    "tfidf[dictionary.doc2bow(\"human computer interaction EPS\".lower().split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The tfidf model again returns a list of tuples, where the first entry is the token ID and the second entry is the tf-idf\n",
    "#weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\lohit\\AppData\\Local\\Temp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os.path\n",
    "\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:35,514 : INFO : loading Dictionary object from C:\\Users\\lohit\\AppData\\Local\\Temp\\den.dict\n",
      "2017-10-25 18:24:35,519 : INFO : loaded C:\\Users\\lohit\\AppData\\Local\\Temp\\den.dict\n",
      "2017-10-25 18:24:35,522 : INFO : loaded corpus index from C:\\Users\\lohit\\AppData\\Local\\Temp\\den.mm.index\n",
      "2017-10-25 18:24:35,525 : INFO : initializing corpus reader from C:\\Users\\lohit\\AppData\\Local\\Temp\\den.mm\n",
      "2017-10-25 18:24:35,529 : INFO : accepted corpus with 9 documents, 35 features, 51 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated before \n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "if os.path.isfile(os.path.join(TEMP_FOLDER, 'den.dict')):\n",
    "    dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'den.dict'))\n",
    "    corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'den.mm'))\n",
    "    print(\"Used files generated before \")\n",
    "else:\n",
    "    print(\"Run again error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "machine\n",
      "interface\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[0])\n",
    "print(dictionary[1])\n",
    "print(dictionary[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:37,005 : INFO : collecting document frequencies\n",
      "2017-10-25 18:24:37,012 : INFO : PROGRESS: processing document #0\n",
      "2017-10-25 18:24:37,014 : INFO : calculating IDF weights for 9 documents and 34 features (51 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5648663441460566), (1, 0.8251824121072071)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5245699338309155), (1, 0.38315779281548723), (2, 0.2622849669154578), (3, 0.38315779281548723), (4, 0.38315779281548723), (5, 0.2622849669154578), (6, 0.38315779281548723)]\n",
      "[(5, 0.3726494271826947), (7, 0.3726494271826947), (8, 0.27219160459794917), (9, 0.5443832091958983), (10, 0.27219160459794917), (11, 0.3726494271826947), (12, 0.3726494271826947)]\n",
      "[(2, 0.438482464916089), (8, 0.32027755044706185), (10, 0.32027755044706185), (13, 0.438482464916089), (14, 0.6405551008941237)]\n",
      "[(0, 0.3449874408519962), (10, 0.5039733231394895), (13, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]\n",
      "[(8, 0.21953536176370683), (11, 0.30055933182961736), (12, 0.30055933182961736), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]\n",
      "[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.48507125007266594), (25, 0.24253562503633297)]\n",
      "[(25, 0.31622776601683794), (26, 0.6324555320336759), (27, 0.31622776601683794), (28, 0.6324555320336759)]\n",
      "[(25, 0.20466057569885868), (27, 0.20466057569885868), (29, 0.2801947048062438), (30, 0.40932115139771735), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.40932115139771735)]\n",
      "[(7, 0.6282580468670046), (27, 0.45889394536615247), (29, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Here we transformed our Tf-Idf corpus via Latent Semantic Indexing into a latent 2-D space (2-D because we set num_topics=2).\n",
    "#Now you’re probably wondering: what do these two latent dimensions stand for? Let’s inspect with models.LsiModel.print_topics():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:39,891 : INFO : using serial LSI version on this node\n",
      "2017-10-25 18:24:39,895 : INFO : updating model with new documents\n",
      "2017-10-25 18:24:39,901 : INFO : preparing a new chunk of documents\n",
      "2017-10-25 18:24:39,904 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-10-25 18:24:39,906 : INFO : 1st phase: constructing (35, 102) action matrix\n",
      "2017-10-25 18:24:39,909 : INFO : orthonormalizing (35, 102) action matrix\n",
      "2017-10-25 18:24:39,916 : INFO : 2nd phase: running dense svd on (35, 9) matrix\n",
      "2017-10-25 18:24:39,920 : INFO : computing the final decomposition\n",
      "2017-10-25 18:24:39,922 : INFO : keeping 2 factors (discarding 66.599% of energy spectrum)\n",
      "2017-10-25 18:24:39,924 : INFO : processed documents up to #9\n",
      "2017-10-25 18:24:39,926 : INFO : topic #0(1.271): 0.408*\"system\" + 0.301*\"survey\" + 0.283*\"user\" + 0.282*\"eps\" + 0.246*\"human\" + 0.236*\"management\" + 0.227*\"opinion\" + 0.226*\"response\" + 0.226*\"time\" + 0.224*\"interface\"\n",
      "2017-10-25 18:24:39,928 : INFO : topic #1(1.180): 0.425*\"minors\" + 0.422*\"graph\" + 0.313*\"survey\" + 0.236*\"trees\" + 0.222*\"intersection\" + 0.222*\"paths\" + 0.188*\"widths\" + 0.188*\"ordering\" + 0.188*\"quasi\" + 0.188*\"well\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the topics are printed to log – see the note at the top of this page about activating logging\n",
    "#It appears that according to LSI, “trees”, “graph” and “minors” are all related words (and contribute the most to the direction\n",
    "#of the first topic), while the second topic practically concerns itself with all the other words. As expected,\n",
    "#the first five documents are more strongly related to the second topic while the remaining four documents to the first topic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:42,179 : INFO : topic #0(1.271): 0.408*\"system\" + 0.301*\"survey\" + 0.283*\"user\" + 0.282*\"eps\" + 0.246*\"human\" + 0.236*\"management\" + 0.227*\"opinion\" + 0.226*\"response\" + 0.226*\"time\" + 0.224*\"interface\"\n",
      "2017-10-25 18:24:42,185 : INFO : topic #1(1.180): 0.425*\"minors\" + 0.422*\"graph\" + 0.313*\"survey\" + 0.236*\"trees\" + 0.222*\"intersection\" + 0.222*\"paths\" + 0.188*\"widths\" + 0.188*\"ordering\" + 0.188*\"quasi\" + 0.188*\"well\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.408*\"system\" + 0.301*\"survey\" + 0.283*\"user\" + 0.282*\"eps\" + 0.246*\"human\" + 0.236*\"management\" + 0.227*\"opinion\" + 0.226*\"response\" + 0.226*\"time\" + 0.224*\"interface\"'),\n",
       " (1,\n",
       "  '0.425*\"minors\" + 0.422*\"graph\" + 0.313*\"survey\" + 0.236*\"trees\" + 0.222*\"intersection\" + 0.222*\"paths\" + 0.188*\"widths\" + 0.188*\"ordering\" + 0.188*\"quasi\" + 0.188*\"well\"')]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.38491889088288589), (1, -0.23799083182688749)]\n",
      "[(0, 0.67327570398106373), (1, 0.061063011983416668)]\n",
      "[(0, 0.59429153367225518), (1, -0.32013947136769177)]\n",
      "[(0, 0.56595474120106282), (1, -0.3443358962412072)]\n",
      "[(0, 0.37883572265509352), (1, -0.013238424814016938)]\n",
      "[(0, 0.032346275240837975), (1, 0.17672649803391668)]\n",
      "[(0, 0.13320822144013733), (1, 0.48928974583068763)]\n",
      "[(0, 0.19468626669806066), (1, 0.6377245136397599)]\n",
      "[(0, 0.37343003126004548), (1, 0.65767275604411801)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:43,985 : INFO : saving Projection object under C:\\Users\\lohit\\AppData\\Local\\Temp\\model.lsi.projection, separately None\n",
      "2017-10-25 18:24:44,000 : INFO : saved C:\\Users\\lohit\\AppData\\Local\\Temp\\model.lsi.projection\n",
      "2017-10-25 18:24:44,002 : INFO : saving LsiModel object under C:\\Users\\lohit\\AppData\\Local\\Temp\\model.lsi, separately None\n",
      "2017-10-25 18:24:44,005 : INFO : not storing attribute projection\n",
      "2017-10-25 18:24:44,007 : INFO : not storing attribute dispatcher\n",
      "2017-10-25 18:24:44,012 : INFO : saved C:\\Users\\lohit\\AppData\\Local\\Temp\\model.lsi\n"
     ]
    }
   ],
   "source": [
    "lsi.save(os.path.join(TEMP_FOLDER, 'model.lsi')) # same for tfidf, lda, ...\n",
    "#lsi = models.LsiModel.load(os.path.join(TEMP_FOLDER, 'model.lsi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A common reason for such a charade is that we want to determine similarity between pairs of documents, or the similarity\n",
    "#between a specific document and a set of other documents (such as a user query vs. indexed documents).\n",
    "#To show how this can be done in gensim,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:45,463 : INFO : loading Dictionary object from C:/Users/lohit/AppData/Local/Temp/den.dict\n",
      "2017-10-25 18:24:45,467 : INFO : loaded C:/Users/lohit/AppData/Local/Temp/den.dict\n",
      "2017-10-25 18:24:45,470 : INFO : loaded corpus index from C:/Users/lohit/AppData/Local/Temp/den.mm.index\n",
      "2017-10-25 18:24:45,473 : INFO : initializing corpus reader from C:/Users/lohit/AppData/Local/Temp/den.mm\n",
      "2017-10-25 18:24:45,475 : INFO : accepted corpus with 9 documents, 35 features, 51 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 35 features, 51 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('C:/Users/lohit/AppData/Local/Temp/den.dict')\n",
    "corpus = corpora.MmCorpus('C:/Users/lohit/AppData/Local/Temp/den.mm') # comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now suppose a user typed in the query “Human computer interaction”. We would like to sort our nine corpus documents in \n",
    "#decreasing order of relevance to this query. Unlike modern search engines, here we only concentrate on a single aspect of\n",
    "#possible similarities—on apparent semantic relatedness of their texts (words). No hyperlinks, no random-walk static ranks,\n",
    "#just a semantic extension over the boolean keyword match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46390054531878433), (1, -0.20359920281643867)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In addition, we will be considering cosine similarity to determine the similarity of two vectors. Cosine similarity is a\n",
    "#standard measure in Vector Space Modeling, but wherever the vectors represent probability distributions, different similarity \n",
    "#measures may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing query structures\n",
    "#To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. \n",
    "#In our case, they are the same nine documents used for training LSI, converted to 2-D LSA space. But that’s only incidental, \n",
    "#we might also be indexing a different corpus altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:49,074 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-10-25 18:24:49,087 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Index persistency is handled via the standard save() and load() functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-25 18:24:50,465 : INFO : saving MatrixSimilarity object under C:/Users/lohit/AppData/Local/Temp/den.index, separately None\n",
      "2017-10-25 18:24:50,472 : INFO : saved C:/Users/lohit/AppData/Local/Temp/den.index\n",
      "2017-10-25 18:24:50,474 : INFO : loading MatrixSimilarity object from C:/Users/lohit/AppData/Local/Temp/den.index\n",
      "2017-10-25 18:24:50,478 : INFO : loaded C:/Users/lohit/AppData/Local/Temp/den.index\n"
     ]
    }
   ],
   "source": [
    "index.save('C:/Users/lohit/AppData/Local/Temp/den.index')\n",
    "index = similarities.MatrixSimilarity.load('C:/Users/lohit/AppData/Local/Temp/den.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is true for all similarity indexing classes (similarities.Similarity, similarities.MatrixSimilarity and\n",
    "#similarities.SparseMatrixSimilarity). Also in the following, index can be an object of any of these. When in doubt, \n",
    "#use similarities.Similarity, as it is the most scalable version, and it also supports adding more documents to the index later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.99145329), (1, 0.8966043), (2, 0.99835241), (3, 0.99378079), (4, 0.9334408), (5, -0.21829586), (6, -0.13329136), (7, -0.10740998), (8, 0.08808583)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar), so that the first document has a\n",
    "#score of 0.99809301 etc.\n",
    "#With some standard Python magic we sort these similarities into descending order, and obtain the final answer to the query \n",
    "#“Human computer interaction”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.99835241), (3, 0.99378079), (0, 0.99145329), (4, 0.9334408), (1, 0.8966043), (8, 0.08808583), (7, -0.10740998), (6, -0.13329136), (5, -0.21829586)]\n"
     ]
    }
   ],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The thing to note here is that documents no. 2 (\"The EPS user interface management system\") and\n",
    "#4 (\"Relation of user perceived response time to error measurement\") would never be returned by a standard boolean fulltext\n",
    "#search, because they do not share any common words with \"Human computer interaction\". \n",
    "#However, after applying LSI, we can observe that both of them received quite high similarity scores \n",
    "#(no. 2 is actually the most similar!), which corresponds better to our intuition of them sharing a “computer-human” \n",
    "#related topic with the query. In fact, this semantic generalization is the reason why we apply transformations and do\n",
    "#topic modelling in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
